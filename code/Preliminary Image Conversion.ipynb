{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../train-test-data'\n",
    "features = ['CHILD_SEX','IDD_SCORE','AGE','HHID_count','HH_AGE','FOOD_EXPENSE_WEEKLY',\n",
    "                 'NON-FOOD_EXPENSE_WEEKLY','HDD_SCORE','FOOD_INSECURITY','YoungBoys','YoungGirls',\n",
    "                 'AverageMonthlyIncome','BEN_4PS','AREA_TYPE','FOOD_EXPENSE_WEEKLY_pc',\n",
    "                 'NON-FOOD_EXPENSE_WEEKLY_pc','AverageMonthlyIncome_pc']\n",
    "train = pd.read_csv(os.path.join(DATA_DIR, '2aii_train.csv'), index_col = 0)\n",
    "test = pd.read_csv(os.path.join(DATA_DIR, '2aii_test.csv'), index_col = 0)\n",
    "label = '2aii'\n",
    "\n",
    "OUT_DIR = '../preliminary-image-data/2aii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Normalizing Variables\n",
    "normalize = ['IDD_SCORE','AGE','HHID_count','HH_AGE','FOOD_EXPENSE_WEEKLY',\n",
    "            'NON-FOOD_EXPENSE_WEEKLY','HDD_SCORE','FOOD_INSECURITY','YoungBoys','YoungGirls',\n",
    "            'AverageMonthlyIncome','FOOD_EXPENSE_WEEKLY_pc','NON-FOOD_EXPENSE_WEEKLY_pc',\n",
    "             'AverageMonthlyIncome_pc']\n",
    "train_normal = train.copy()\n",
    "test_normal = test.copy()\n",
    "for f in normalize:\n",
    "    train_normal[f] = sigmoid((train_normal[f]-train_normal[f].mean())/train_normal[f].std())\n",
    "    test_normal[f] = sigmoid((test_normal[f]-test_normal[f].mean())/test_normal[f].std())\n",
    "\n",
    "train_normal['BEN_4PS'] = train_normal['BEN_4PS']-1 \n",
    "test_normal['BEN_4PS'] = test_normal['BEN_4PS']-1 \n",
    "\n",
    "train_normal['label'] = np.where(train_normal['2aii']==\"INCREASED RISK\", 1, 0)\n",
    "test_normal['label'] = np.where(test_normal['2aii']==\"INCREASED RISK\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(features)\n",
    "w, h = IMG_SIZE\n",
    "nw = 5\n",
    "nh = (n+nw-1)//nw\n",
    "\n",
    "for index, row in train_normal.iterrows():\n",
    "    img = Image.new(\"RGB\", IMG_SIZE)\n",
    "    for i in range(0,nh):\n",
    "        for j in range(0,nw):\n",
    "            idx = i*nw+j\n",
    "            if idx >= n: break \n",
    "            val = int(sigmoid(row[features[idx]])*255)\n",
    "\n",
    "            r = ImageDraw.Draw(img)\n",
    "            x = i*(h//nh)\n",
    "            y = j*(w//nw)\n",
    "            r.rectangle([(y,x), (y+w//nw, x+h//nh)], fill=(val,val,val))\n",
    "\n",
    "    img.save(os.path.join(OUT_DIR,'train', str(row['label']), f'{index}.png'))\n",
    "\n",
    "for index, row in test_normal.iterrows():\n",
    "    img = Image.new(\"RGB\", IMG_SIZE)\n",
    "    for i in range(0,nh):\n",
    "        for j in range(0,nw):\n",
    "            idx = i*nw+j\n",
    "            if idx >= n: break \n",
    "            val = int(sigmoid(row[features[idx]])*255)\n",
    "\n",
    "            r = ImageDraw.Draw(img)\n",
    "            x = i*(h//nh)\n",
    "            y = j*(w//nw)\n",
    "            r.rectangle([(y,x), (y+w//nw, x+h//nh)], fill=(val,val,val))\n",
    "\n",
    "    img.save(os.path.join(OUT_DIR,'test', str(row['label']), f'{index}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 303 files belonging to 2 classes.\n",
      "Found 303 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(os.path.join(OUT_DIR, 'train'),\n",
    "                                                       shuffle=True,\n",
    "                                                       batch_size=8,\n",
    "                                                       image_size=IMG_SIZE)\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(os.path.join(OUT_DIR, 'test'),\n",
    "                                                       shuffle=True,\n",
    "                                                       batch_size=8,\n",
    "                                                       image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAF+CAYAAACPnGAaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAAsTAAALEwEAmpwYAAAME0lEQVR4nO3dTYvd5RnH8es2I8QkmjibRiYECbaCLdhFF4IvIGbVtUWxCxFSBbcusgjiW+jCneCiK+2qq7wAN7pQKPgAwUJqQiQkTs2DiczdhRGaB8cUcuY6/s7nswpnFtd1Jv878+V/zpmMOWcBACR5oHsBAID7TeAAAHEEDgAQR+AAAHEEDgAQR+AAAHEEDgAQR+AswBjjtTHGh2OM78YY73TvA92cCbiVM7F4a90LhPqqqt6qqqNV9VDzLrAMnAm4lTOxYAJnAeac71dVjTH+UFWHmteBds4E3MqZWDwvUQEAcQQOABBH4AAAcQQOABDHm4wXYIyxVj98b3dV1a4xxu6q+n7O+X3vZtDDmYBbOROL5w7OYpyoqqtV9UZVvXDzzydaN4JezgTcyplYsDHn7N4BAOC+cgcHAIgjcACAOAIHAIgjcACAOAIHAIiz7e/BOXnyZOtHrI4cOdI5vh54oK//5pz19ddft82vqnrqqada5z/33HOjdYG76D4T6+vrneNb5885a3Nzs23+Mnj11VeX7kycOnWq7UzMOevy5ctd42vOWZcuXWqd/80337TNr6rat29f6/xXXnnlJ8+EOzgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEWeteYDtzTvObjDHanz/LxzXB7bqvie753fyc+GnbBs7Bgwd3ao+7unr1auv83bt3t82ec7ZeOHPO+vTTT9vmV1UdO3asdf7dbGxstM1ehmvi7NmzbfOrqh577LHW+Z3/JiyrL7/8snX+nj17Vnb+nLMuXrzYOv/KlStt83+Ol6gAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCII3AAgDgCBwCIs7bdF+ecO7XH0hljrPTzZ/kswzXZOX/Vn/+yWvXvyao//2W2beBcu3Ztp/a4q127drXNnnPWuXPn2uZXVT377LNts+ecdfbs2bb5y+rAgQNts+ecrWdyGc7E6dOnW+cfPny4df4yunLlSuv8jY2Nttlzzvroo4/a5lf1/5w4f/582/yf4yUqACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4ggcACCOwAEA4qx1LwC/JHPO7hUAuAfbBs6ePXt2ao87zDnrxo0bbfPHGLW+vl4XLlxo26H7h2n3/GV08eLF1vn79u1rnf/EE0/UF1980bpDJ2fiTocPH26d/+CDD7bOf+aZZ+qDDz5om999TXbP387Ybrm33367bfPuwPlxh9dff711h1U25xzdO9yu80xU9QfOnLNefPHF1h1W2TKeiffee2/lz8SxY8dad1hl250J78EBAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgjsABAOIIHAAgzphzdu8AAHBfuYMDAMQROABAHIEDAMQROABAHIEDAMQROABAHIEDAMQROABAHIEDAMQROABAHIEDAMQROABAHIEDAMQROABAHIEDAMQROABAHIGzAGOM18YYH44xvhtjvNO9D3QbY6yPMf4+xrg8xvjXGONP3TsB2da6Fwj1VVW9VVVHq+qh5l1gGfy1qq5X1a+q6vdV9Y8xxsdzzn+2bgXEGnPO7h1ijTHeqqpDc84/d+8CXcYYe6vqYlX9bs75+c3H3q2qf88532hdDojlJSpg0X5TVd//GDc3fVxVv23aB1gBAgdYtH1VtXnbY99U1cMNuwArQuAAi/ZtVT1y22OPVNV/GnYBVoTAARbt86paG2P8+n8ee7qqvMEYWBiBswBjjLUxxu6q2lVVu8YYu8cYPrHGSppzXq6q96vqzTHG3jHGs1X1x6p6t3czIJnAWYwTVXW1qt6oqhdu/vlE60bQ6y/1w69MOF9Vf6uq4z4iDiySj4kDAHHcwQEA4ggcACCOwAEA4ggcACCOwAEA4mz7u1lOnjzZ+hGrp59+um32nLNu3LjROn9ra6t1/tWrV9vmV1W9/PLLo3WBuzh16lTbmZhz1ieffNI1vqqqDh482DZ7zlmbm7f/jw87a319vXX+888/v3RnApaVOzgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEETgAQByBAwDEWeteYDtzTvNZKqv+d9L9/DvnjzHanz9w77YNnEOHDu3UHneYc9aFCxfa5ldVPfzww22z55y1tbXVNr+qav/+/a3zl9GZM2faZs8569FHH22d/+2337bOv3btWtv8qqrr16+3zgfunZeoAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4AgcAiLPWvcBPGWPU1tZW6/xO3fOrquac3Sssne7vifmuSeDebBs4+/fv36k97jDnrHPnzrXNr6p6/PHHW+d3mnPW6dOnu9dYOpubm63z9+7d2zZ7zlnnz59vm78Mjhw50r0CcI+8RAUAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AECcte2+OOfcqT2W0pyzxhjdawAA/6dtA+fSpUs7tMadliEuPvvss3ryySdbd+i06oF7NwcOHGibPeesra2ttvlVVRsbG3XmzJnWHTo5E/DLsW3grLo5Zx09erR7jZV1/Pjx7hW4izfffLN7hZX10ksvda8AvxjegwMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AEAcgQMAxBE4AECcMefs3gEA4L5yBwcAiCNwAIA4AgcAiCNwAIA4AgcAiCNwAIA4/wWxsSpfoLQv3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "rescale = tf.keras.layers.Rescaling(1./127.5, offset=-1)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 5, 5, 1280)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_ds))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1280)\n",
      "(8, 1)\n"
     ]
    }
   ],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)\n",
    "\n",
    "prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy', \n",
    "                       tf.keras.metrics.TruePositives(),\n",
    "                       tf.keras.metrics.TrueNegatives(),\n",
    "                       tf.keras.metrics.FalsePositives(),\n",
    "                       tf.keras.metrics.FalseNegatives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 160, 160, 3)]     0         \n",
      "                                                                 \n",
      " tf.math.truediv_4 (TFOpLamb  (None, 160, 160, 3)      0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " tf.math.subtract_4 (TFOpLam  (None, 160, 160, 3)      0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " mobilenetv2_1.00_160 (Funct  (None, 5, 5, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d_5   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 1,862,721\n",
      "Non-trainable params: 396,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 4s 21ms/step - loss: 0.4936 - accuracy: 0.8218 - true_positives_8: 248.0000 - true_negatives_5: 1.0000 - false_positives_5: 51.0000 - false_negatives_5: 3.0000\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 1s 17ms/step - loss: 0.5118 - accuracy: 0.8185 - true_positives_8: 248.0000 - true_negatives_5: 0.0000e+00 - false_positives_5: 52.0000 - false_negatives_5: 3.0000\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.5009 - accuracy: 0.8251 - true_positives_8: 250.0000 - true_negatives_5: 0.0000e+00 - false_positives_5: 52.0000 - false_negatives_5: 1.0000\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 1s 17ms/step - loss: 0.4838 - accuracy: 0.8284 - true_positives_8: 250.0000 - true_negatives_5: 1.0000 - false_positives_5: 51.0000 - false_negatives_5: 1.0000\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 1s 17ms/step - loss: 0.4907 - accuracy: 0.8218 - true_positives_8: 249.0000 - true_negatives_5: 0.0000e+00 - false_positives_5: 52.0000 - false_negatives_5: 2.0000\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 1s 17ms/step - loss: 0.4769 - accuracy: 0.8185 - true_positives_8: 248.0000 - true_negatives_5: 0.0000e+00 - false_positives_5: 52.0000 - false_negatives_5: 3.0000\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.4850 - accuracy: 0.8251 - true_positives_8: 250.0000 - true_negatives_5: 0.0000e+00 - false_positives_5: 52.0000 - false_negatives_5: 1.0000\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.4778 - accuracy: 0.8284 - true_positives_8: 251.0000 - true_negatives_5: 0.0000e+00 - false_positives_5: 52.0000 - false_negatives_5: 0.0000e+00\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 1s 17ms/step - loss: 0.4631 - accuracy: 0.8284 - true_positives_8: 250.0000 - true_negatives_5: 1.0000 - false_positives_5: 51.0000 - false_negatives_5: 1.0000\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.4648 - accuracy: 0.8284 - true_positives_8: 251.0000 - true_negatives_5: 0.0000e+00 - false_positives_5: 52.0000 - false_negatives_5: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4643292725086212 0.8250824809074402 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_ds, verbose=0)\n",
    "loss, accuracy, tp, tn, fp, fn = scores\n",
    "sensitivity = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "print(loss, accuracy, sensitivity, specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisPU",
   "language": "python",
   "name": "thesispu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
